{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HathiTrust US Federal Document Suggester Analysis\n",
    "### *Analysis of missing USFD in known USFDs using the HT US Fed Docs Registry*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "HathiTrust (HT) determines US Federal Documents (USFDs) through bibliographic analysis. Specifically, the 008 is inspected for a designation it was published in the US and is a federal document*. Many institutions have deposited USFDs in the HT repository but have yet to indicate them through the 008 in the MARC record. If they were to update their metdata records with the correct indicators, many of these volumes would be bibliographically determined to be public domain. This new determination would give the public access through the HT web site. \n",
    "\n",
    "We can identify many of these unmarked USFDs by using the HT US Federal Documents registry, which maintains a database of OCLC numbers of US Federal Documents. By cross-referencing this database with the OCLC numbers in Zephir's HT metadata database, we can find volumes which are not marked as a USFD, but probably should be. From this we can derive lists to give to contributors suggesting records to inspect and update if they truly are USFDs.\n",
    "\n",
    "This notebook analysis this overlap and creates a prototype for generating suggestion lists.\n",
    "\n",
    "*This is a proxy for US Federal Documents, as sometimes the US government publshes documents outside of the US and other countries may puplish documents in the US."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import collections\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zephir/USFD Registry Overlap\n",
    "This is a dataset of overlap between two data sources (Zephir and HT USFD Registry). It was created by [jupyter notebook](http://localhost:8888/lab/tree/create_overlap_dataset.ipynb) matching on exports from each source. OCLC was used as a matchpoint between the sources. Since hathitrust volumes can have more than one OCLC number, it is possible for a volume to be duplicated in this overlap dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zephir-htusfd-overlap was created on 2019-09-03T19:01:42.875874\n",
      "The dataset was derived from:\n",
      "Dataset zephir-oclc-feddocs-dataset.csv created at 2019-09-03T19:00:53.931199\n",
      "Dataset feddoc_oclc_nums.txt created at 2019-09-03T14:47:29\n"
     ]
    }
   ],
   "source": [
    "# gunzip large file if needed\n",
    "with gzip.open(\"data/zephir-htusfd-overlap.csv.gz\", \"rb\") as f_in:\n",
    "    with open(\"data/zephir-htusfd-overlap.csv\", \"wb\") as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "with open('data/zephir-htusfd-overlap.manifest.json') as json_file:\n",
    "    overlap_manifest = json.load(json_file)\n",
    "    \n",
    "print(\"The {} was created on {}\".format(overlap_manifest[\"name\"], overlap_manifest[\"datetime\"]))\n",
    "print(\"The dataset was derived from:\")\n",
    "for dataset in overlap_manifest[\"data-origins\"]:\n",
    "    print(\"Dataset {} created at {}\".format(dataset[\"origin\"],dataset[\"datetime\"]))\n",
    "\n",
    "usfd_overlap_df = pandas.read_csv(\"data/zephir-htusfd-overlap.csv\", dtype=overlap_manifest[\"schema\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many volumes in HathiTrust have overlapping OCLCs in the US Fed Docs Registry?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1227577 volumes w/ OCLC numbers matched in the registry\n"
     ]
    }
   ],
   "source": [
    "# only count unique volumes, some may have multiple oclcs and matches\n",
    "volumes_matched = usfd_overlap_df.htid.unique()\n",
    "print(\"There are {} volumes w/ OCLC numbers matched in the registry\".format(len(volumes_matched)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many of those volumes are not marked USFD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 235359 volumes not marked USFD\n"
     ]
    }
   ],
   "source": [
    "# Only keep records not marked as USFD\n",
    "usfd_suggest_df = usfd_overlap_df[usfd_overlap_df['usfd_status']==False]\n",
    "# Drop OCLC and eliminate duplicates caused by multiple OCLC matches\n",
    "unique_usfd_suggest_df = usfd_suggest_df.drop(columns=\"oclc\").drop_duplicates(keep=\"first\")\n",
    "print(\"There are {} volumes not marked USFD\".format(len(unique_usfd_suggest_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the breakdown of those volumes by contributor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contributors by volumes\n",
      "                   htid\n",
      "contributor_code       \n",
      "AEU                 172\n",
      "CHI                2139\n",
      "CMALG               108\n",
      "COO               13565\n",
      "CTU                  33\n",
      "DEU                   1\n",
      "DUL                  35\n",
      "FMU                   1\n",
      "FU                  667\n",
      "GEU                  10\n",
      "HVD                6767\n",
      "IAU                 708\n",
      "IEN                6281\n",
      "INU                5268\n",
      "LOC                 438\n",
      "MCHB                 15\n",
      "MDU                   1\n",
      "MIEM               1146\n",
      "MIU               70777\n",
      "MMET                  5\n",
      "MOU                   7\n",
      "MU                  387\n",
      "NCSU                 11\n",
      "NJP                1307\n",
      "NNC                1172\n",
      "NYP                2092\n",
      "OU                 9095\n",
      "PST               16451\n",
      "PU                   11\n",
      "PUR                 235\n",
      "QMM                   1\n",
      "TXCM                 36\n",
      "TXU                3362\n",
      "UC                43397\n",
      "UIU               26960\n",
      "UMN               12549\n",
      "UNC                  96\n",
      "USU                   1\n",
      "UVA                6415\n",
      "WAU                   6\n",
      "WU                 3509\n",
      "YALE                122\n"
     ]
    }
   ],
   "source": [
    "print(\"Contributors by volumes\")\n",
    "print(unique_usfd_suggest_df[[\"contributor_code\", \"htid\"]].groupby([\"contributor_code\"]).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many bibliographic records submitted from ILS systems are not marked USFD? \n",
    "\n",
    "There can be multiple volulmes associated with a single bibliographic record, so this count will be lower. Ultimately, to update the 008, they contributor will need to us the ILS biblographic identifier rather than the HT volume identifier. We call this identifier the 'contribsys_id' in the Zephir database. They will then need to update that biblographic record in their ILS and resubmit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count by contribsys_id: 129331\n"
     ]
    }
   ],
   "source": [
    "# Drop HTID and eliminate duplicates from 1-N relationship with contribsys_id (Contributor's ILS Bib #)\n",
    "unique_contribsys_usfd_df = unique_usfd_suggest_df.drop(columns=[\"htid\"]).drop_duplicates(keep=\"first\")\n",
    "print(\"Total count by contribsys_id: {}\".format(len(unique_contribsys_usfd_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the contributor summary by bibliographic control identifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contributors by contribsys_id (ILS Bib #)\n",
      "                  contribsys_id\n",
      "contributor_code               \n",
      "AEU                         172\n",
      "CHI                         666\n",
      "CMALG                        93\n",
      "COO                        5946\n",
      "CTU                          30\n",
      "DEU                           1\n",
      "DUL                          34\n",
      "FMU                           1\n",
      "FU                          267\n",
      "GEU                          10\n",
      "HVD                        3826\n",
      "IAU                         502\n",
      "IEN                        4678\n",
      "INU                        3720\n",
      "LOC                         382\n",
      "MCHB                         15\n",
      "MDU                           1\n",
      "MIEM                        592\n",
      "MIU                       37044\n",
      "MMET                          5\n",
      "MOU                           7\n",
      "MU                          225\n",
      "NCSU                          9\n",
      "NJP                         637\n",
      "NNC                         781\n",
      "NYP                        1016\n",
      "OU                         4291\n",
      "PST                        4915\n",
      "PU                            1\n",
      "PUR                         118\n",
      "QMM                           1\n",
      "TXCM                         36\n",
      "TXU                        2409\n",
      "UC                        24422\n",
      "UIU                       19455\n",
      "UMN                        6516\n",
      "UNC                          90\n",
      "USU                           1\n",
      "UVA                        3946\n",
      "WAU                           6\n",
      "WU                         2367\n",
      "YALE                         97\n"
     ]
    }
   ],
   "source": [
    "print(\"Contributors by contribsys_id (ILS Bib #)\")\n",
    "print(unique_contribsys_usfd_df[[\"contributor_code\", \"contribsys_id\"]].groupby([\"contributor_code\"]).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving dataset\n",
    "\n",
    "Export candidate suggestions to a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presort the output\n",
    "sorted_contribsys_suggestions_df = unique_contribsys_usfd_df.sort_values(by=[\"contributor_code\", \"contribsys_id\"])\n",
    "export_df = unique_contribsys_usfd_df.drop(columns=\"usfd_status\")\n",
    "\n",
    "dataset_name = \"htusfd-suggestions\"\n",
    "dataset_file = \"data/{}.csv\".format(dataset_name)\n",
    "\n",
    "# save the dataset\n",
    "export_df.to_csv(\"data/{}.csv\".format(dataset_name), header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a manifest\n",
    "manifest = {}\n",
    "manifest[\"name\"] = dataset_name\n",
    "manifest[\"description\"] = \"Dataset for identifying possible unmarked USFD by contributor\"\n",
    "manifest[\"datetime\"] = str(datetime.datetime.utcfromtimestamp(os.path.getmtime(\"data/{}.csv\".format(manifest[\"name\"]))).isoformat())\n",
    "\n",
    "# derive schema from dataframe\n",
    "schema = collections.OrderedDict()\n",
    "for column in export_df.columns.array:\n",
    "    schema[str(column)]= str(export_df.dtypes[column])\n",
    "manifest[\"schema\"] = schema\n",
    "\n",
    "# not format\n",
    "manifest[\"format\"] = {\n",
    "        \"delimiter\": \",\",\n",
    "        \"encoding\": \"utf8\",\n",
    "        \"extension\": \"csv\",\n",
    "        \"header\": True,\n",
    "        \"type\": \"delimited\"\n",
    "    }\n",
    "        \n",
    "# note the origins\n",
    "manifest[\"data-origins\"] = [{\n",
    "    \"origin\": \"zephir-htusfd-overlap.csv\",\n",
    "    \"datetime\": str(overlap_manifest[\"datetime\"])}]\n",
    "\n",
    "# save the manifest\n",
    "manifest_file = \"data/{}.manifest.json\".format(manifest[\"name\"])\n",
    "with open(manifest_file, 'w') as outfile:\n",
    "    json.dump(manifest, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finishing up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed notebook (2019-09-03T19:04:04.784883).\n",
      "Output created:\n",
      "data/htusfd-suggestions.csv\n",
      "data/htusfd-suggestions.manifest.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Completed notebook ({}).\".format(datetime.datetime.utcnow().isoformat()))\n",
    "print(\"Output created:\")\n",
    "print(dataset_file)\n",
    "print(manifest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
